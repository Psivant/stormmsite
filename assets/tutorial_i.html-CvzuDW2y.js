import{_ as e,c as n,o as t,b as a}from"./app-BKGJkkpB.js";const s={},i=a(`<h1 id="a-basic-gpu-program-in-stormm" tabindex="-1"><a class="header-anchor" href="#a-basic-gpu-program-in-stormm"><span>A Basic GPU Program in STORMM</span></a></h1><p>Begin at the beginning. A GPU program will launch kernels on the device for massive parallel processing in a <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" target="_blank" rel="noopener noreferrer">Non-Uniform Memory Access (NUMA)</a> framework. From the perspective of the host, one CPU thread will stage data on the GPU, copying data from one resource to another, then launch kernels to process that data. A serial CPU process is the administrator for a series of workhorse GPU processes, and this is reflected in the C++ to CUDA transition that STORMM facilitates for NVIDIA hardware.</p><p>It starts with one array. Constants can be sent to the GPU as kernel launch parameters, but results from those kernel must, in general, be written to memory that the GPU can access (e.g. its on-board memory, or <a href="https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/html/group__CUDART__MEMORY_g15a3871f15f8c38f5b7190946845758c.html" target="_blank" rel="noopener noreferrer">memory on the host allocated in such a way that the GPU can see it</a>. The contents of the array can then be downloaded by the CPU in the same way that the CPU can upload data to the device. STORMM encapsulates memoory management through the templated <code>Hybrid</code> C++ class.</p><p>The first step in writing the program is to recognize the GPU. STORMM provides several classes to identify and select all available GPUs on the system. They are the <code>GpuDetails</code> and <code>HpcConfig</code>, available by including the following header files. When STORMM is compiled for High Performance Computing (HPC) and the <a href="https://developer.nvidia.com/cuda-zone" target="_blank" rel="noopener noreferrer">NVIDIA Compute Unified Device Architecture (CUDA)</a> framework, the <code>GpuDetails</code> header includes the requisite <code>cuda_runtime.h</code> library, but we can replicate that below for clarity:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#ifdef STORMM_USE_HPC</span>
<span class="line">#  ifdef STORMM_USE_CUDA</span>
<span class="line">#    include &lt;cuda_runtime.h&gt;</span>
<span class="line">#  endif</span>
<span class="line">#endif</span>
<span class="line">#include &quot;/stormm/home/src/card/gpu_details.h&quot;</span>
<span class="line">#include &quot;/stormm/home/src/card/hpc_config.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>We also need access to the <code>Hybrid</code> template implementation and some enumerators to choose among different options, which are available through:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/card/gpu_enumerators.h&quot;</span>
<span class="line">#include &quot;/stormm/home/src/card/hybrid.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>Whenever designing a GPU program, the purpose is speed of execution, and a means of profiling the code is therefore helpful. While <a href="https://docs.nvidia.com/cuda/profiler-users-guide/" target="_blank" rel="noopener noreferrer">NVIDIA&#39;s <code>nvprof</code> tool</a> is useful for measuring kernel execution speed, for many applications a great deal of time may be spent on the CPU, staging the calculation. If the GPU accelerates by 1000x something which is 99% of the run time of a single-threaded, CPU program, then in a run that would have taken 100 seconds that portion takes 0.099 seconds in a GPU-enabled code, but the CPU staging or post-processing portion still takes one second, accounting for 90% of the wall time. It can also be useful for users to see the wall time of various parts of their calculations, to optimize their own workflows or provide feedback to the developers. For generating timings &quot;in the wild&quot;, one cannot reply on a profiler, and therefore STORMM provides a dedicated class, the <code>StopWatch</code>, for timing different segments of a program. Include:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/UnitTesting/stopwatch.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>These capabilities, to find a GPU and to initialize a timings apparatus, can be laid out in the opening lines of the program. Once the GPU is found, it is also helpful to do something such as allocate an array on the device, to trigger NVIDIA firmware and prepare the card for use by the program.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  StopWatch the_clock(&quot;STORMM Tutorial I&quot;);</span>
<span class="line">  const int gpu_asgn_tm = the_clock.addCategory(&quot;Assign a GPU&quot;);</span>
<span class="line">  const int gpu_prep_tm = the_clock.addCategory(&quot;Prep the GPU&quot;);</span>
<span class="line">  the_clock.assignTime();</span>
<span class="line">#ifdef STORMM_USE_HPC</span>
<span class="line">  const HpcConfig gpu_config(ExceptionResponse::WARN);</span>
<span class="line">  const std::vector&lt;int&gt; my_gpus = gpu_config.getGpuDevice(1);</span>
<span class="line">  const GpuDetails gpu = gpu_config.getGpuInfo(my_gpus[0]);</span>
<span class="line">  the_clock.assignTime(&quot;Assign a GPU&quot;);</span>
<span class="line">  Hybrid&lt;int&gt; force_gpu_to_engage(1);</span>
<span class="line">#  ifdef STORMM_USE_CUDA</span>
<span class="line">  cudaDeviceSynchronize();</span>
<span class="line">#  endif</span>
<span class="line">  the_clock.assignTime(&quot;Prep the GPU&quot;);</span>
<span class="line">  printf(&quot;This program is running on a %s card:\\n&quot;, gpu.getCardName().c_str());</span>
<span class="line">  printf(&quot;  Major.minor architecture version: %d.%d\\n&quot;, gpu.getArchMajor(), gpu.getArchMinor());</span>
<span class="line">  printf(&quot;  Streaming multiprocessors: %d\\n&quot;, gpu.getSMPCount());</span>
<span class="line">  printf(&quot;  Card RAM: %d megabtyes\\n&quot;, gpu.getCardRam());</span>
<span class="line">  printf(&quot;  Global cache size: %d bytes\\n\\n&quot;, gpu.getGlobalCacheSize());</span>
<span class="line">#endif</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The above code demonstrates how the <code>GpuDetails</code> class, is a standard C++ class with constructors and accessors. It can be returned by methods in other classes or passed to functions that will manage GPU kernel launches. Moreover, it is a wrapper for various structs in CUDA or, potentially, other HPC languages that capture specs of a card in the computer system.</p><p>The <code>StopWatch</code> class is intended to give developers simple and efficient methods to assign the time spent between any two wall time measurements to one of many customized, labeled bins. The code above demonstrates assignment based on the name of the bin. As implied by the variable <code>gpu_asgn_time</code> and <code>gpu_prep_time</code> variables, each category in the time tracking is given a unique integer value, which can also be used to immediately get the right bin rather than some search over name strings.</p><p>Aside from the tivial array which was allocated to force the GPU to engage, we can now create a more substantial array and manipulate its contents. The <code>Hybrid</code> class will only accept template types of familiar, elemental types such as <code>int</code>, <code>double</code>, <code>char</code>, or <code>bool</code>. It is not like the C++ Standard Template Library <code>std::vector</code>, which can be a container for arrays of custom classes and typically has optimized implementations for <code>std::vector&lt;bool&gt;</code>. Some of the basic methods are similar, and for this STORMM uses &quot;camel case&quot; rather than underscore-separated methods, to help developers see when they are dealing with the STORMM dynamic memory class rather than the C++ standard. The subscript array index operator <code>[]</code> is not overloaded in <code>Hybrid</code> objects at this time. One feature of the <code>Hybrid</code> class is a developer-defined label that goes along with each array, which will be displayed if range-checked memory accesses fail to expedite backtracing.</p><p>Let us allocate an array to hold a number sequence. The sequence will start at some value and then ping-pong between two limits.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  // Create an array of integers on the CPU host and (if available) on the GPU device</span>
<span class="line">  const int int_experiment_tm = the_clock.addCategory(&quot;Experiment with integers&quot;);</span>
<span class="line">  Hybrid&lt;int&gt; xferable_integers(128, &quot;Test_Int_Array&quot;);</span>
<span class="line">  int ri = -5;</span>
<span class="line">  for (size_t i = 0; i &lt; xferable_integers.size(); i++) {</span>
<span class="line">    xferable_integers.putHost(ri, i);</span>
<span class="line">    if (i == 0) {</span>
<span class="line">      ri++;</span>
<span class="line">    }</span>
<span class="line">    else if (ri == 16) {</span>
<span class="line">      ri -= 2;</span>
<span class="line">    }</span>
<span class="line">    else if (ri == -8) {</span>
<span class="line">      ri++;</span>
<span class="line">    }</span>
<span class="line">    else {</span>
<span class="line">      if (xferable_integers.readHost(i - 1) &lt; ri) {</span>
<span class="line">        ri++;</span>
<span class="line">      }</span>
<span class="line">      else {</span>
<span class="line">        ri -= 2;</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="abstracts-pointers-to-the-data" tabindex="-1"><a class="header-anchor" href="#abstracts-pointers-to-the-data"><span>Abstracts: Pointers to the Data</span></a></h2><p>This is an opportunity to demonstrate the C++ to C, C to CUDA strategy that guides much of STORMM&#39;s development. C++ made big improvements in its 2011 update, among them compilers getting smart about seeing the subscript array operator on <code>std::vector</code> objects and optimizing the pointer arithmetic to work at the same speed as the original C implementations. CUDA needs pointers to work best, not classes with methods implemented on the CPU. The common space on the Venn diagram is, therefore, to take a valid pointer to the data in an array and access data as if the program were written in C. We need to set the pointer again if the array is resized and do not know just by looking at the pointer how much valid memory is allocated behind it, but the most common bugs with arrays involve requesting an array subscript index that is out of bounds, and <code>std::vector</code> will not check the index anyway. We will use this method to print all contents of the array in a human-readable format:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  // Grab a pointer to the array&#39;s host-side data and print the contents</span>
<span class="line">  const int* host_xi_ptr = xferable_integers.data(HybridTargetLevel::HOST);</span>
<span class="line">  const int nxi = xferable_integers.size();</span>
<span class="line">  printf(&quot;Contents of xferable_integers:\\n  &quot;);</span>
<span class="line">  for (int i = 0; i &lt; nxi; i++) {</span>
<span class="line">    printf(&quot; %3d&quot;, host_xi_ptr[i]);</span>
<span class="line">    if ((i + 1) % 16 == 0 || i == nxi - 1) {</span>
<span class="line">      printf(&quot;   (%3d)\\n  &quot;, i + 1);</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">  printf(&quot;\\n&quot;);</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>This strategy of creating pointers and length constants to traverse arrays is taken to a higher degree in the class abstract approach found throughout much of STORMM, as will be evident in later tutorials.</p><h2 id="analysis-of-the-data-on-the-cpu-and-the-gpu" tabindex="-1"><a class="header-anchor" href="#analysis-of-the-data-on-the-cpu-and-the-gpu"><span>Analysis of the Data on the CPU and the GPU</span></a></h2><p>Next, we can consider some operation that involves the array as a whole: the sum of all elements. This is trivial to do in a single-threaded C program by looping over each element, as was done above to display the values. In general, we must consider cases where the array might be so large that the data type used to hold the summation might break: a floating point value could overflow to <code>NaN</code> or <code>Inf</code>, but more likely a 32-bit <code>int</code> could be overwhelmed by combining a million values in the range of 10,000. We don&#39;t need to worry with the problem at hand, but this sort of safety is what drives a lot of the details in general-purpose code.</p><p>Summing the contents of an array in CUDA is very much like taking their sum in another parallel programming scheme: divide the problem into non-overlapping parts, delegate, and then combine the results. It&#39;s not within the scope of the present tutorial to delve into the particulars of kernel writing, which is also covered on many other forums. This is an opportunity to talk about how STORMM deals with code bloat and makes C++ templating amenable to CUDA in light of the above vulnerability, which necessitates versatility in the choice of data types. A typical C++ program cannot launch a CUDA kernel unless it was compiled directly by the NVIDIA CUDA compiler. STORMM is built to work in a CPU-only environment, to compile without the benefits of CUDA, so that development can continue and features that do not involve massive number crunching remain accessible on machines without compatible GPUs.</p><h2 id="templating-and-the-transition-from-c-to-cuda" tabindex="-1"><a class="header-anchor" href="#templating-and-the-transition-from-c-to-cuda"><span>Templating and the Transition from C++ to CUDA</span></a></h2><p>In order to launch a CUDA kernel, we must write a separate CUDA unit, with a <strong>.cu</strong> rather than <strong>.cpp</strong> extension. The CUDA unit will also be able to understand templated CUDA kernels, files that STORMM stuffs into files with <strong>.cuh</strong> extensions (while many C++ programmers write templated functions in the header files where the function is first mentioned, STORMM takes a convention of creating a separate <strong>.tpp</strong> file for the implementation as opposed to the declaration). The <strong>.cu</strong> file will contain the implementation of a function to take an array of a given data type and compute the sum. For complete generality, we would again need to allow that the sum be computed in a distnct data type, longer in format than the type of the array elements, but we can assume the same type for each in this example. The C++ program cannot be told to <code>#include</code> any CUDA template implementations--the function within the <strong>.cu</strong> file will take an array and then delegate to some templated form of a CUDA kernel that only the CUDA unit knows about. C++ doesn&#39;t know what a kernel, much less a template implementation for a kernel, really is. However, the C++ program will <code>#include</code> the header describing the launching function, as illustrated in the following diagram:</p><p>In the same way that a C++ program cannot <code>#include</code> CUDA code, in our case critical headers for templated kernels, the CUDA unit will not be able to accept templated inputs from the C++ compiled code object file. Rather, we must convert all pointers to a specific type to pass through the C++ : CUDA barrier, then undo the conversion on the other side. This is done via the universal C pointer type, <code>void*</code>. We will convert the pointers for our array of integers, as well as the buffer array we are using to hold the answers from each GPU thread block, to <code>void*</code>, then codify the <code>int</code> data type and pass that as another input parameter to the CUDA unit. The type index of the <code>int</code> data type is computed by STORMM at runtime and is available through the <code>data_types</code> library. As a final condition, we must take the pointer to our array&#39;s data on the GPU device, not on the CPU host. Because the data in the integer array is not intended to change during the summation, we will <code>const</code> qualify that pointer as well as its <code>void*</code> alias. (Aside, it&#39;s OK to feed the <code>int*</code> pointers to a function that expects to take <code>void*</code>. The conversion will be automatic, although the tutorial avoids this shorthand for clarity.) A summary of the code:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/DataTypes/common_types.h&quot;</span>
<span class="line"></span>
<span class="line">  // Allocate a buffer for the answer</span>
<span class="line">  Hybrid&lt;int&gt; sum_of_xi(gpu.getSMPCount(), &quot;Our_Answer&quot;);</span>
<span class="line">  const int* devc_xi_ptr = xferable_integers.data(HybridTargetLevel::DEVICE);</span>
<span class="line">  const void* vdevc_xi_ptr = reinterpret_cast&lt;const void*&gt;(devc_xi_ptr);</span>
<span class="line">  int* sum_ptr = sum_of_xi.data(HybridTargetLevel::DEVICE);</span>
<span class="line">  void* vsum_ptr = reinterpret_cast&lt;const void*&gt;(sum_ptr);  </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>All that remains on the C++ side is to stage the data on the GPU and then issue a call to the function that will launch the kernel (a memory transfer to or from the GPU can be called from a C++ compiled code object, even though a kernel cannot). The relevant code:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">xferable_integers.upload();</span>
<span class="line">wrapTheSummationLaunch(vdevc_xi_ptr, nxi, vsum_ptr, int_type_index, gpu);</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="the-cuda-side-of-things" tabindex="-1"><a class="header-anchor" href="#the-cuda-side-of-things"><span>The CUDA Side of Things</span></a></h2><p>The code for our CUDA unit is not complex. It must define an implementation for the wrapper function <code>wrapTheSummationLaunch</code>, which branches over various recognized arithemtic types to be summed:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">extern void wrapTheSummationLaunch(const void* vdata, const size_t n, void* vresult,</span>
<span class="line">                                   const size_t ct_data, const GpuDetails &amp;gpu) {</span>
<span class="line">  if (ct_data == int_type_index) {</span>
<span class="line">    const int* data = reinterpret_cast&lt;const int*&gt;(vdata);</span>
<span class="line">    int* result = reinterpret_cast&lt;int*&gt;(vresult);</span>
<span class="line">    kSumVector&lt;int, int&gt;&lt;&lt;&lt;gpu.getSMPCount(), large_block_size&gt;&gt;&gt;(data, n, result);</span>
<span class="line">  }</span>
<span class="line">  else if (ct_data == llint_type_index) {</span>
<span class="line">    const llint* data = reinterpret_cast&lt;const llint*&gt;(vdata);</span>
<span class="line">    llint* result = reinterpret_cast&lt;llint*&gt;(vresult);</span>
<span class="line">    kSumVector&lt;llint, llint&gt;&lt;&lt;&lt;gpu.getSMPCount(), large_block_size&gt;&gt;&gt;(data, n, result);</span>
<span class="line">  }</span>
<span class="line">  else if (ct_data == double_type_index) {</span>
<span class="line">    const double* data = reinterpret_cast&lt;const double*&gt;(vdata);</span>
<span class="line">    double* result = reinterpret_cast&lt;double*&gt;(vresult);</span>
<span class="line">    kSumVector&lt;double, double&gt;&lt;&lt;&lt;gpu.getSMPCount(), large_block_size&gt;&gt;&gt;(data, n, result);</span>
<span class="line">  }</span>
<span class="line">  else if (ct_data == float_type_index) {</span>
<span class="line">    const float* data = reinterpret_cast&lt;const float*&gt;(vdata);</span>
<span class="line">    float* result = reinterpret_cast&lt;float*&gt;(vresult);</span>
<span class="line">    kSumVector&lt;float, float&gt;&lt;&lt;&lt;gpu.getSMPCount(), large_block_size&gt;&gt;&gt;(data, n, result);</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Note the presence of the <code>extern</code> qualifier in the CUDA implementation file. This does not apply to the function declaration in the associated header file.</p><p>The sum of the contents in our array may be computed on the host as well. It is 522. To check the result, we can use one of STORMM&#39;s built-in vector math functions:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/Math/vector_ops.h&quot;</span>
<span class="line"></span>
<span class="line">  printf(&quot;The sum of the set of integers on the host is:          %d\\n&quot;, sum&lt;int&gt;(host_xi_ptr));</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The summation is overloaded to accept a Hybrid object, a <code>std::vector&lt;T&gt;</code>, or a C-style array with a trusted length. Many other vector-applicable functions work the same way. For summations in particular, if a valid <code>GpuDetails</code> object is provided, the summation will run over the data present on the GPU device, with the option to have a temporary buffer array created just for that process (no need for manual allocation of <code>sum_of_xi</code> as we did above). However, overloads of the <code>sum</code> function that accept GPU inputs are currently only callable from CUDA units. Other overloads are callable from both C++ and CUDA code, as is seen above. One other thing to note is that the <code>sum</code> function does take two formal template arguments, one for the data and another for the type to store the running sum--the data type is inferred from the input, while the type of the running sum is provided in the <code>sum&lt;int&gt;</code> call.</p><h2 id="the-stopwatch-for-tracking-wall-time" tabindex="-1"><a class="header-anchor" href="#the-stopwatch-for-tracking-wall-time"><span>The <code>StopWatch</code> for Tracking Wall Time</span></a></h2><p>As a final analysis, we can add lines to check the timing of various operations. The various categories we laid out can take contributions as has been shown. The <code>StopWatch</code> class relies on the <a href="https://pubs.opengroup.org/onlinepubs/009604599/functions/gettimeofday.html" target="_blank" rel="noopener noreferrer">ANSI-C <code>gettimeofday()</code></a> function and has a precision of microseconds. To summarize:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;../../src/UnitTesting/stopwatch.h&quot;</span>
<span class="line"></span>
<span class="line">  Stopwatch the_clock(&quot;STORMM Tutorial I&quot;);</span>
<span class="line">  const int gpu_asgn_tm = the_clock.addCategory(&quot;Assign a GPU&quot;);</span>
<span class="line">  const int gpu_prep_tm = the_clock.addCategory(&quot;Prep the GPU&quot;);</span>
<span class="line">  const int int_experiment_tm = the_clock.addCategory(&quot;Experiment with integers&quot;);</span>
<span class="line">  </span>
<span class="line">  // Assign time since last measurement based on category name (slower, use only in high-level</span>
<span class="line">  // procedures)</span>
<span class="line">  the_clock.assignTime(&quot;Assign a GPU&quot;);</span>
<span class="line"></span>
<span class="line">  // Assign time since last measurement based on category index (faster, use when taking repeated</span>
<span class="line">  // samples during an inner loop)</span>
<span class="line">  the_clock.assignTime(int_experiment_tm);</span>
<span class="line"></span>
<span class="line">  // Assign time since last measurement to the default &quot;miscellaneous&quot; category (specify index 0,</span>
<span class="line">  // or call with no argument)</span>
<span class="line">  the_clock.assignTime();</span>
<span class="line">  the_clock.assignTime(0);</span>
<span class="line">  </span>
<span class="line">  // Print the timings results (precision optional)</span>
<span class="line">  the_clock.printResults();</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The program discussed in this tutorial can be found in the following files:</p><ul><li><strong>/stormm/home/apps/Tutorial/tutorial_i.cpp</strong> (C++ implementation)</li><li><strong>/stormm/home/apps/Tutorial/hpc_tutorial_i.cu</strong> (CUDA unit)</li><li><strong>/stormm/home/apps/Tutorial/hpc_tutorial_i.h</strong> (header for inclusion by the C++ program)</li></ul><p>The templated implementations for C++ summation can be found in <strong>/stormm/home/src/Math/summation.tpp</strong> (included by <strong>/stormm/home/apps/Tutorial/tutorial_i.cpp</strong>), while templated implementations for CUDA kernels are found in <strong>/stormm/home/src/Math/hpc_summation.cuh</strong> (included by <strong>/stormm/home/apps/Tutorial/hpc_tutorial_i.cu</strong>)</p><p>The tutorial is one of the applications compiled with the basic STORMM build, and can be run on the command line:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">&gt;&gt; /stormm/build/dir/apps/Tutorial/tutorial_i.stormm.cuda</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>This tutorial</p>`,46),l=[i];function o(r,d){return t(),n("div",null,l)}const p=e(s,[["render",o],["__file","tutorial_i.html.vue"]]),u=JSON.parse('{"path":"/tutorials/tutorial_i.html","title":"A Basic GPU Program in STORMM","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Abstracts: Pointers to the Data","slug":"abstracts-pointers-to-the-data","link":"#abstracts-pointers-to-the-data","children":[]},{"level":2,"title":"Analysis of the Data on the CPU and the GPU","slug":"analysis-of-the-data-on-the-cpu-and-the-gpu","link":"#analysis-of-the-data-on-the-cpu-and-the-gpu","children":[]},{"level":2,"title":"Templating and the Transition from C++ to CUDA","slug":"templating-and-the-transition-from-c-to-cuda","link":"#templating-and-the-transition-from-c-to-cuda","children":[]},{"level":2,"title":"The CUDA Side of Things","slug":"the-cuda-side-of-things","link":"#the-cuda-side-of-things","children":[]},{"level":2,"title":"The StopWatch for Tracking Wall Time","slug":"the-stopwatch-for-tracking-wall-time","link":"#the-stopwatch-for-tracking-wall-time","children":[]},{"level":2,"title":"Summary","slug":"summary","link":"#summary","children":[]}],"git":{"updatedTime":1748449380000},"filePathRelative":"tutorials/tutorial_i.md"}');export{p as comp,u as data};
