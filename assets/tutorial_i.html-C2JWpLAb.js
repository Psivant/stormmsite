import{_ as e,c as n,o as a,b as t}from"./app-BtJcHWfP.js";const s={},i=t(`<h1 id="a-basic-gpu-program-in-stormm" tabindex="-1"><a class="header-anchor" href="#a-basic-gpu-program-in-stormm"><span>A Basic GPU Program in STORMM</span></a></h1><p>Begin at the beginning. A GPU program will launch kernels on the device for massive parallel processing in a <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" target="_blank" rel="noopener noreferrer">Non-Uniform Memory Access (NUMA)</a> framework. From the perspective of the host, one CPU thread will stage data on the GPU, copying data from one resource to another, then launch kernels to process that data. A serial CPU process is the administrator for a series of workhorse GPU processes, and this is reflected in the C++ to CUDA transition that STORMM facilitates for NVIDIA hardware.</p><p>It begins with one array. Constants can be sent to the GPU as kernel launch parameters, but results from a CUDA kernel must, in general, be written to memory that the GPU can access (e.g. its on-board memory, or <a href="https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/html/group__CUDART__MEMORY_g15a3871f15f8c38f5b7190946845758c.html" target="_blank" rel="noopener noreferrer">memory on the host allocated in such a way that the GPU can see it</a>. The contents of the array can then be downloaded by the CPU in the same way that the CPU can upload data to the device. STORMM encapsulates memoory management through the templated <code>Hybrid</code> C++ class.</p><p>The first step in writing the program is to recognize the GPU. STORMM provides several classes to identify and select all available GPUs on the system. They are the <code>GpuDetails</code> and <code>HpcConfig</code>, available by including the following header files:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/card/gpu_details.h&quot;</span>
<span class="line">#include &quot;/stormm/home/src/card/hpc_config.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>We also need access to the <code>Hybrid</code> template implementation and some enumerators to choose among different options, which are available through:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/card/gpu_enumerators.h&quot;</span>
<span class="line">#include &quot;/stormm/home/src/card/hybrid.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>Whenever designing a GPU program, the purpose is speed of execution, and a means of profiling the code is therefore helpful. While <a href="https://docs.nvidia.com/cuda/profiler-users-guide/" target="_blank" rel="noopener noreferrer">NVIDIA&#39;s <code>nvprof</code> tool</a> is useful for measuring kernel execution speed, for many applications a great deal of time may be spent on the CPU, staging the calculation. If the GPU accelerates by 1000x something which is 99% of the run time of a single-threaded, CPU program, then in a run that would have taken 100 seconds that portion takes 0.099 seconds in a GPU-enabled code, but the CPU staging or post-processing portion still takes one second, accounting for 90% of the wall time. It can also be useful for users to see the wall time of various parts of their calculations, to optimize their own workflows or provide feedback to the developers. For generating timings &quot;in the wild&quot;, one cannot reply on a profiler, and therefore STORMM provides a dedicated class, the <code>StopWatch</code>, for timing different segments of a program. Include:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">#include &quot;/stormm/home/src/UnitTesting/stopwatch.h&quot;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>These capabilities, to find a GPU and to initialize a timings apparatus, can be laid out in the opening lines of the program. Once the GPU is found, it is also helpful to do something such as allocate an array on the device, to trigger NVIDIA firmware and prepare the card for use by the program.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  StopWatch the_clock(&quot;STORMM Tutorial I&quot;);</span>
<span class="line">  const int gpu_asgn_tm = the_clock.addCategory(&quot;Assign a GPU&quot;);</span>
<span class="line">  const int gpu_prep_tm = the_clock.addCategory(&quot;Prep the GPU&quot;);</span>
<span class="line">  the_clock.assignTime();</span>
<span class="line">#ifdef STORMM_USE_HPC</span>
<span class="line">  const HpcConfig gpu_config(ExceptionResponse::WARN);</span>
<span class="line">  const std::vector&lt;int&gt; my_gpus = gpu_config.getGpuDevice(1);</span>
<span class="line">  const GpuDetails gpu = gpu_config.getGpuInfo(my_gpus[0]);</span>
<span class="line">  the_clock.assignTime(&quot;Assign a GPU&quot;);</span>
<span class="line">  Hybrid&lt;int&gt; force_gpu_to_engage(1);</span>
<span class="line">#  ifdef STORMM_USE_CUDA</span>
<span class="line">  cudaDeviceSynchronize();</span>
<span class="line">#  endif</span>
<span class="line">  the_clock.assignTime(&quot;Prep the GPU&quot;);</span>
<span class="line">  printf(&quot;This program is running on a %s card:\\n&quot;, gpu.getCardName().c_str());</span>
<span class="line">  printf(&quot;  Major.minor architecture version: %d.%d\\n&quot;, gpu.getArchMajor(), gpu.getArchMinor());</span>
<span class="line">  printf(&quot;  Streaming multiprocessors: %d\\n&quot;, gpu.getSMPCount());</span>
<span class="line">  printf(&quot;  Card RAM: %d megabtyes\\n&quot;, gpu.getCardRam());</span>
<span class="line">  printf(&quot;  Global cache size: %d bytes\\n\\n&quot;, gpu.getGlobalCacheSize());</span>
<span class="line">#endif</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The above code demonstrates how the <code>GpuDetails</code> class, is a standard C++ class with constructors and accessors. It can be returned by methods in other classes or passed to functions that will manage GPU kernel launches. Moreover, it is a wrapper for various structs in CUDA or, potentially, other HPC languages that capture specs of a card in the computer system.</p><p>The <code>StopWatch</code> class is intended to give developers simple and efficient methods to assign the time spent between any two wall time measurements to one of many customized, labeled bins. The code above demonstrates assignment based on the name of the bin. As implied by the variable <code>gpu_asgn_time</code> and <code>gpu_prep_time</code> variables, each category in the time tracking is given a unique integer value, which can also be used to immediately get the right bin rather than some search over name strings.</p><p>Aside from the tivial array which was allocated to force the GPU to engage, we can now create a more substantial array and manipulate its contents. The <code>Hybrid</code> class will only accept template types of familiar, elemental types such as <code>int</code>, <code>double</code>, <code>char</code>, or <code>bool</code>. It is not like the C++ Standard Template Library <code>std::vector</code>, which can be a container for arrays of custom classes and typically has optimized implementations for <code>std::vector&lt;bool&gt;</code>. Some of the basic methods are similar, and for this STORMM uses &quot;camel case&quot; rather than underscore-separated methods, to help developers see when they are dealing with the STORMM dynamic memory class rather than the C++ standard. The subscript array index operator <code>[]</code> is not overloaded in <code>Hybrid</code> objects at this time. One feature of the <code>Hybrid</code> class is a developer-defined label that goes along with each array, which will be displayed if range-checked memory accesses fail to expedite backtracing.</p><p>Let us allocate an array to hold a number sequence. The sequence will start at some value and then ping-pong between two limits.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  // Create an array of integers on the CPU host and (if available) on the GPU device</span>
<span class="line">  const int int_experiment_tm = the_clock.addCategory(&quot;Experiment with integers&quot;);</span>
<span class="line">  Hybrid&lt;int&gt; xferable_integers(128, &quot;Test_Int_Array&quot;);</span>
<span class="line">  int ri = -5;</span>
<span class="line">  for (size_t i = 0; i &lt; xferable_integers.size(); i++) {</span>
<span class="line">    xferable_integers.putHost(ri, i);</span>
<span class="line">    if (i == 0) {</span>
<span class="line">      ri++;</span>
<span class="line">    }</span>
<span class="line">    else if (ri == 16) {</span>
<span class="line">      ri -= 2;</span>
<span class="line">    }</span>
<span class="line">    else if (ri == -8) {</span>
<span class="line">      ri++;</span>
<span class="line">    }</span>
<span class="line">    else {</span>
<span class="line">      if (xferable_integers.readHost(i - 1) &lt; ri) {</span>
<span class="line">        ri++;</span>
<span class="line">      }</span>
<span class="line">      else {</span>
<span class="line">        ri -= 2;</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>This is an opportunity to demonstrate the C++ to C, C to CUDA strategy that guides much of STORMM&#39;s development. C++ made big improvements in its 2011 update, among them compilers getting smart about seeing the subscript array operator on <code>std::vector</code> objects and optimizing the pointer arithmetic to work at the same speed as the original C implementations. CUDA needs pointers to work best, not classes with methods implemented on the CPU. The common space on the Venn diagram is, therefore, to take a valid pointer to the data in an array and access data as if the program were written in C. We need to set the pointer again if the array is resized and do not know just by looking at the pointer how much valid memory is allocated behind it, but the most common bugs with arrays involve requesting an array subscript index that is out of bounds, and <code>std::vector</code> will not check the index anyway. We will use this method to print all contents of the array in a human-readable format:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">  // Grab a pointer to the array&#39;s host-side data and print the contents</span>
<span class="line">  const int* host_xi_ptr = xferable_integers.data(HybridTargetLevel::HOST);</span>
<span class="line">  const int nxi = xferable_integers.size();</span>
<span class="line">  printf(&quot;Contents of xferable_integers:\\n  &quot;);</span>
<span class="line">  for (int i = 0; i &lt; nxi; i++) {</span>
<span class="line">    printf(&quot; %3d&quot;, host_xi_ptr[i]);</span>
<span class="line">    if ((i + 1) % 16 == 0 || i == nxi - 1) {</span>
<span class="line">      printf(&quot;   (%3d)\\n  &quot;, i + 1);</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">  printf(&quot;\\n&quot;);</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Next, we can consider some operation that invovles the array as a whole: the sum of all elements. This is trivial to do in a single-threaded C program by looping over each element, as was done above to display the values. In general, we must consider cases where the array might be so large that the data type used to hold the summation might break: a floating point value could overflow to <code>NaN</code> or <code>Inf</code>, but more likely a 32-bit <code>int</code> could be overwhelmed by combining a million values in the range of 10,000. We don&#39;t need to worry with the problem at hand, but this sort of safety is what drives a lot of the details in general-purpose code.</p><p>Summing the contents of an array in CUDA is very much like taking their sum in another parallel programming scheme: divide the problem into non-overlapping parts, delegate, and then combine the results. It&#39;s not within the scope of the present tutorial to delve into the particulars of kernel writing, which is also covered on many other forums. This is an opportunity to talk about how STORMM deals with code bloat and makes C++ templating amenable to CUDA in light of the above vulnerability, which necessitates versatility in the choice of data types. A typical C++ program cannot launch a CUDA kernel unless it was compiled directly by the NVIDIA CUDA compiler. STORMM is built to work in a CPU-only environment, to compile without the benefits of CUDA, so that development can continue and features that do not involve massive number crunching remain accessible on machines without compatible GPUs.</p><p>In order to launch a CUDA kernel, we must write a separate CUDA unit, with a <strong>.cu</strong> rather than <strong>.cpp</strong> extension. The CUDA unit will also be able to understand templated CUDA kernels, files that STORMM stuffs into files with <strong>.cuh</strong> extensions (while many C++ programmers write templated functions in the header files where the function is first mentioned, STORMM takes a convention of creating a separate <strong>.tpp</strong> file for the implementation as opposed to the declaration). The <strong>.cu</strong> file will contain the implementation of a function to take an array of a given data type and compute the sum. For complete generality, we would again need to allow that the sum be computed in a distnct data type, longer in format than the type of the array elements, but we can assume the same type for each in this example. The C++ program cannot be told to <code>#include</code> any CUDA template implementations--the function within the <strong>.cu</strong> file will take an array and then delegate to some templated form of a CUDA kernel that only the CUDA unit knows about. C++ doesn&#39;t know what a kernel, much less a template implementation for a kernel, really is. However, the C++ program will <code>#include</code> the header describing the launching function, as illustrated in the following diagram:</p>`,21),o=[i];function r(l,c){return a(),n("div",null,o)}const p=e(s,[["render",r],["__file","tutorial_i.html.vue"]]),h=JSON.parse('{"path":"/tutorials/tutorial_i.html","title":"A Basic GPU Program in STORMM","lang":"en-US","frontmatter":{},"headers":[],"git":{"updatedTime":1748407408000},"filePathRelative":"tutorials/tutorial_i.md"}');export{p as comp,h as data};
